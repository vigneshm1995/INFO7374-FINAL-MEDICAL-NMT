{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "med-data-clean-preprocess.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "colab_type": "text",
        "id": "J0Qjg6vuaHNt"
      },
      "cell_type": "markdown",
      "source": [
        "We are using code from [Tensorflow NMT Tutorial](https://www.tensorflow.org/alpha/tutorials/text/nmt_with_attention) to do most of the preprocessing"
      ]
    },
    {
      "metadata": {
        "id": "7wEV1QxdW3Q3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Clean Data"
      ]
    },
    {
      "metadata": {
        "id": "OO4PawR4ih0i",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "1. Add a *start* and *end* token to each sentence.\n",
        "2. Clean the sentences by removing special characters.\n",
        "3. Create a word index and reverse word index (dictionaries mapping from word → id and id → word).\n",
        "4. Pad each sentence to a maximum length."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "rd0jw-eC3jEh",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Converts the unicode file to ascii\n",
        "def unicode_to_ascii(s):\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "\n",
        "def preprocess_sentence(w):\n",
        "    w = unicode_to_ascii(w.lower().strip())\n",
        "    \n",
        "    # creating a space between a word and the punctuation following it\n",
        "    # eg: \"he is a boy.\" => \"he is a boy .\" \n",
        "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
        "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "    w = re.sub(r'[\" \"]+', \" \", w)\n",
        "    \n",
        "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
        "    \n",
        "    w = w.rstrip().strip()\n",
        "    \n",
        "    # adding a start and an end token to the sentence\n",
        "    # so that the model know when to start and stop predicting.\n",
        "    w = '<start> ' + w + ' <end>'\n",
        "    \n",
        "    return w"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "OHn4Dct23jEm",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 1. Remove the accents\n",
        "# 2. Clean the sentences\n",
        "# 3. Return word pairs in the format: [ENGLISH, FRENCH]\n",
        "\n",
        "import zipfile\n",
        "def create_dataset(path_raw, path_target, num_examples):\n",
        "  \n",
        "    with zipfile.ZipFile(path_raw, 'r') as zip_ref:\n",
        "      zip_ref.extractall('')\n",
        "    \n",
        "    with open(path_target, encoding = 'UTF-8') as f:\n",
        "      lines = f.read().strip().split('\\n')\n",
        "      each_line = []\n",
        "      for i in lines:\n",
        "        each_line.append(i.split(\"|\"))\n",
        "        \n",
        "      src_data = []\n",
        "      tgt_data = []\n",
        "      for i in range(len(each_line)-1):\n",
        "        the_tuple = each_line[i][1]\n",
        "        if(the_tuple!=\"[In Process Citation].\" and the_tuple!=\"[Not Available].\"):\n",
        "          src_data.append(each_line[i][1].strip('[].').lower())\n",
        "          tgt_data.append(each_line[i][2].strip('[].').lower())\n",
        "    \n",
        "    src = [preprocess_sentence(s) for s in src_data[:num_examples]]\n",
        "    tgt = [preprocess_sentence(s) for s in tgt_data[:num_examples]]\n",
        "    \n",
        "    # save a list of clean sentences to file\n",
        "    def save_clean_txt(filename, dataset): \n",
        "      with open(filename, 'x') as f:\n",
        "        for line in dataset:\n",
        "          f.write(line)\n",
        "          f.write(\"\\n\") \n",
        "      print('Saved: {}'.format(filename))  \n",
        "    \n",
        "    save_clean_txt('en_clean.txt', src)\n",
        "    save_clean_txt('fr_clean.txt', tgt)\n",
        "    \n",
        "    return src, tgt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "if5UUFdIyuhw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# path_raw = 'drive/My Drive/INFO7374-NeuralNetwork&AI/Final-Project/data/med/pubmed_en_fr.txt.zip'\n",
        "# path_data = 'pubmed_en_fr.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j25sacLpdbwy",
        "colab_type": "code",
        "outputId": "036bd991-458d-4693-f61f-156afb40bd35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "# src, tgt = create_dataset(path_raw, path_data, None)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved: en_clean.txt\n",
            "Saved: fr_clean.txt\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}